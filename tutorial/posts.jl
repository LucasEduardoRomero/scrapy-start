{"title": "A practical guide to web data QA part IV: Complementing semi-automated techniques", "first_text": "change,deep-thoughts,thinking,world"}
{"title": "Announcing the Web Data Extraction Summit 2020", "first_text": "Web data extraction has become one of the most important tools for businesses to grow and stay ahead of the competition. From developing better pricing strategies to identifying hidden risks and building better products, web data extraction provides the power to transform infinite web data into a structured format that can help you make profitable decisions."}
{"title": "News & article data extraction: Open source vs closed source solutions", "first_text": "Article extraction is the process of extracting data fields from an article page and putting it into a machine-readable structured format like JSON. In many use cases, the article page that you want to extract is a news page but it can be any other type of article. Based on our experience in the web data extraction industry for over 10 years, the demand for structured article data is getting higher. There is more information available on the internet than ever. But still, having access to structured news data and being able to consume relevant and timely information can set you apart and give you a competitive edge. This is what article extraction can do for you."}
{"title": "Announcing the Web Data Extraction Summit 2020", "first_text": "Web data extraction has become one of the most important tools for businesses to grow and stay ahead of the competition. From developing better pricing strategies to identifying hidden risks and building better products, web data extraction provides the power to transform infinite web data into a structured format that can help you make profitable decisions."}
{"title": "Hi, we\u2019re Zyte!", "first_text": null}
{"title": "Irish data extraction innovator unveils new name \u2013 and new AI-powered Automatic Extraction service", "first_text": "February 2nd, 2021, Scrapinghub, a global leader in web data extraction technology and services, today becomes "}
{"title": "Scrapy update: Better broad crawl performance", "first_text": "When crawling the web, there\u2019s always a speed limit. A spider can't fetch faster than the host willing to send him pages. Page serving takes some amount of resources - CPU, disk, network bandwidth, etc. These resources cost money. Unrestricted serving and extensive crawling are the worst combinations. Such a combination could bring applications to halt and deny service to users. Taking all this into account, limiting serving capacity is natural."}
{"title": "A practical guide to web data QA part V: Broad crawls", "first_text": null}
{"title": "Announcing The Web Data Extraction Summit 2020", "first_text": "Web data extraction has become one of the most important tools for businesses to grow and stay ahead of the competition. From developing better pricing strategies to identifying hidden risks and building better products, web data extraction provides the power to transform infinite web data into a structured format that can help you make profitable decisions."}
{"title": "Large scale web scraping", "first_text": "From inconsistent website layouts that break our extraction logic to badly written HTML, web scraping comes with its share of difficulties. Over the last few years, the single most important challenge in web scraping has been to actually get to the data - and not get blocked. This is due to the antibots or the underlying technologies that websites use to protect their data. Proxies are a major component in any scalable web scraping infrastructure. However, not many people understand the technicalities of the different types of proxies and how to make the best use of proxies to get the data they want, with the least possible blocks."}
{"title": "Web scraping basics: A developer\u2019s guide to reliably extract data", "first_text": "The web is complex and constantly changing. It is one of the reasons why web data extraction can be difficult, especially in the long term. It\u2019s necessary to understand how a website works really well before you try to extract data. Luckily, there are lots of inspection and code tools available for this and in this article, we will show you some of our favorites."}
{"title": "Zyte Automatic Extraction job postings API: Stable release", "first_text": "We are excited to announce our newest Automatic Extraction API. The "}
{"title": "Extracting article & news data: The importance of data quality", "first_text": "Article and news data extraction is becoming increasingly popular and widely used by companies. Data quality plays a vital role in making sure these projects succeed. If the quality of the extracted articles is not good enough, your whole business could be at risk, especially if it depends on the constant flow of high-quality article data."}
{"title": "How to get high success rates with proxies: 3 steps to scale up", "first_text": "In this article, we give you some insight on how you can scale up your web data extraction project. You will learn what are the basic elements of scaling up and what are the steps that you should take when looking for the best "}
{"title": "Your price intelligence questions answered", "first_text": "Price Intelligence is leveraging web data to make better pricing, marketing, and business decisions. Basically, it is all about making use of the available data to optimize your pricing strategy, making it more competitive, increasing profitability, and ultimately, improving your business performance."}
{"title": "Price gouging or economics at work: Price intelligence to track consumer sentiment", "first_text": "As the COVID-19 pandemic took hold, we at Zyte (formally Scrapinghub) began to wonder how it would impact the data we crawl, and whether that data could tell us something useful about the pandemic and its impact."}
{"title": "Data center proxies vs. residential proxies", "first_text": "In this blog post, you are going to learn the main difference between data center proxies and residential proxies. When to use data center and residential proxies in your web data extraction project to maximize successful requests"}
{"title": "Blog comments API (BETA): Extract blog comment data at scale", "first_text": "We are excited to announce our newest Automatic Extraction API. The "}
{"title": "Scrapy Cloud secrets: Hub Crawl Frontier and how to use it", "first_text": "Imagine a long crawling process, like extracting data from a website for a whole month. We can start it and leave it running until we get the results. Though, we can agree that a whole month is plenty of time for something to go wrong. The target website can go down for a few minutes/hours, there can be some sort of power outage in your crawling server or even some other internet connection issues."}
{"title": "Real estate: Use web data extraction to make smarter decisions", "first_text": "As the internet continues to grow, the amount of data it generates grows with it, opening new opportunities to improve processes and make more informed decisions. Real estate is one of the many industries that are being disrupted by data-related technologies and innovations. Whether you are a broker, realtor, investor, or property manager you have the potential to become data-driven and gain invaluable insights from web extracted data."}
{"title": "A practical guide to web data extraction QA part II: Common validation pitfalls", "first_text": null}
{"title": "Vehicle API (beta): Extract automotive data at scale", "first_text": "Today we are delighted to launch a beta of our newest data extraction API: "}
{"title": "Transitioning to remote working as a company", "first_text": "I\u2019d like to echo Joel Gasgoine\u2019s sentiments: This is not normal remote working!"}
{"title": "Job postings beta API: Extract job postings at scale", "first_text": "We\u2019re excited to announce our newest data extraction API, "}
{"title": "COVID-19: Handling the situation as a fully remote company", "first_text": "Zyte (formerly Scrapinghub) is a fully distributed organization with a remote workforce spread across the globe. This structure will enable us to continue to operate at full capacity during the Coronavirus pandemic and deliver full service to our customers."}
{"title": "A practical guide to web data QA part I: Validation techniques", "first_text": "When it comes to web scraping at scale, there\u2019s a set of challenges you need to overcome to extract the data. But once you are able to get it, you still have work to do. You need to have a data QA process in place. Data quality becomes especially crucial if you\u2019re extracting high volumes of data from the web regularly and your team\u2019s success depends on the quality of the scraped data."}
{"title": "Extracting clean article HTML with news API", "first_text": "The Internet offers a"}
{"title": "A practical guide to web data QA part III: Holistic data validation techniques", "first_text": null}
{"title": "Custom crawling & News API: designing a web scraping solution", "first_text": "Web scraping projects usually involve data extraction from many websites. The standard approach to tackle this problem is to write some code to navigate and extract the data from each website. However, this approach may not scale so nicely in the long-term, requiring maintenance effort for each website; it also doesn\u2019t scale in the short-term, when we need to start the extraction process in a couple of weeks. Therefore, we need to think of different solutions to tackle these issues."}
{"title": "Product reviews API (beta): Extract product reviews at scale", "first_text": "We are excited to announce our next "}
{"title": "Introducing Zyte Smart Proxy Manager (formerly Crawlera) \u2014 free trials & new plans", "first_text": "One of the biggest pain points we\u2019ve heard from our Zyte Smart Proxy Manager (formerly Crawlera) customers last year is the inconvenience of having to jump from one Zyte Smart Proxy Manager plan to another when more requests are needed in a month. For this reason, we have been working on rethinking our Zyte Smart Proxy Manager plans to better accommodate these cases and be more flexible with customers that have variable crawling requirements from month to month."}
{"title": "Looking back at 2019", "first_text": "2019 was an exciting year for Zyte (formerly Scrapinghub). We created things we have never created before and did things nobody in our industry had ever done before. Let\u2019s revisit what happened in 2019!"}
{"title": "Backconnect proxy: Explanation & comparison to other proxies", "first_text": "Scaling up your web scraping project is not an easy task. Adding proxies is one of the first actions you will need to take. You will need to manage a healthy proxy pool to avoid bans. There are a lot of proxy services/providers, each having a whole host of different types of proxies. In this blog post, you are going to learn how backconnect proxies work and when you should use them."}
{"title": "4 sectors that benefited most from business intelligence software", "first_text": "Data moves around the marketplace. It can be sourced internally or externally and collected from vendors, manufacturers, retailers, wholesalers, consumers, and other players in the marketplace. This data is then processed and used by businesses in making insights and decisions regarding new business ventures, product ideas, conflict resolution, and process improvement."}
{"title": "How to leverage alternative data in asset management", "first_text": "Whether you are managing a hedge fund trying to find innovative sources of alpha or are an analyst looking to future proof your company\u2019s financial investments, as big data continues to disrupt the investment research landscape, getting on top of these alternative datasets as early as possible is the key to capturing the immense alpha left in this data."}
{"title": "Best recruitment tips: How to scout top talent", "first_text": "Attracting top talent is essential for the success and growth of a company. The majority of employers will agree that finding the best talent is just as hard as it is important. Which is why, rather than waiting for the right candidate to magically fall into your lap, it's time for employers to turn towards the untapped power of "}
{"title": "How to use a proxy in Puppeteer", "first_text": " is a high-level API for headless chrome. It\u2019s one of the most popular tools to use for web automation or web scraping in Node.js. In web scraping, many developers use it to handle javascript rendering and web data extraction. In this article, we are going to cover how to set up a proxy in Puppeteer and what your options are if you want to rotate proxies."}
{"title": "News web data extraction to predict Irish election results", "first_text": "On February 9th, 2020, Ireland elected a new parliament. Prior to the elections, the political parties invested a lot of time, money and energy to get their political message to the people. A lot of research goes into selecting the right platform and the right medium."}
{"title": "How to scrape the web without getting blocked", "first_text": "Web scraping is when you extract data from the web and put it in a structured format. Getting structured data from publicly available websites and pages should not be an issue as everybody with an internet connection can access these websites. You should be able to structure it as well. In reality, though, it\u2019s not that easy."}
{"title": "Building spiders made easy: GUI for your Scrapy shell", "first_text": "As a python developer at Zyte (formerly Scrapinghub), I spend a lot of time in the Scrapy shell. This is a command-line interface that comes with Scrapy and allows you to run simple, spider compatible code. It gets the job done, sure, but there\u2019s a point where a command-line interface can become kinda fiddly and I found I was passing that point pretty regularly. I have some background in tool design and task automation so I naturally began to wonder how I could improve my experience and help focus more time on building great spiders. Over my Christmas break, I dedicate some free time to improve this experience, resulting in my first python package "}
{"title": "The Web Data Extraction Summit 2019", "first_text": "The Web Data Extraction Summit was held last week, on 17th September, in Dublin, Ireland. This was the first-ever event dedicated to web scraping and data extraction. We had over 140 curious attendees, 16 great speakers from technical deep dives to business use cases, 12 amazing presentations, a customer panel discussion, and unlimited Guinness."}
{"title": "Scrapy & Zyte Automatic Extraction API integration", "first_text": "We\u2019ve just released a "}
{"title": "Web scraping questions & answers part I", "first_text": "As you know we held the "}
{"title": "Web scraping questions & answers part II", "first_text": ", we answered some of the best questions we got during "}
{"title": "Gain a competitive edge with product data", "first_text": "Product data\u2014whether from e-commerce sites, auto listings, or product reviews\u2014offers a treasure trove of insights that can give your business an immense competitive edge in your market. Getting access to this data in a structured format can unleash new potential for not only business intelligence teams, but also their counterparts in marketing, sales, and management that rely on accurate data to make mission-critical business decisions."}
{"title": "News data extraction at scale with AI-powered Zyte Automatic Extraction", "first_text": "A huge portion of the internet is news. It\u2019s a very important type of content because there are always things happening either in our local area or globally that we want to know about. The amount of news published every day on different sites is ridiculous. Sometimes it\u2019s good news and sometimes it\u2019s bad news but one thing\u2019s for sure: it\u2019s humanly impossible to read all of it every day."}
{"title": "Price intelligence with Python: Scrapy, SQL, and Pandas", "first_text": "In this article, I will guide you through a web scraping and data visualization project. We will extract e-commerce data from real e-commerce websites then try to get some insights out of it. The goal of this article is to show you how to get product pricing data from the web and what are some ways to analyze pricing data. We will also look at how price intelligence makes a real difference for e-commerce companies when making pricing decisions."}
{"title": "Scrapy, Matplotlib, and MySQL: Real estate data analysis", "first_text": "In this article, we will extract real estate listings from one of the biggest real estate sites and then analyze the data. Similar to our previous "}
{"title": "How to use Zyte Smart Proxy Manager (formerly Crawlera) with Scrapy", "first_text": " a "}
{"title": "Price scraping: The best free tool to scrape prices", "first_text": "Price scraping is something that you need to do if you want to extract pricing data from websites. It might look easy and just a minor technical detail that needs to be handled but in reality, if you don\u2019t know the best way to get those price values from the HTMLs, it can be a headache over time."}
{"title": "GDPR update: Scraping public personal data", "first_text": "One common misconception about scraping personal data is that public personal data does not fall under the GDPR. Many businesses assume that because the data has already been made public on another website that it is fair game to scrape. In actuality, GDPR makes no blanket exceptions for public personal data and the same analysis for any other personal data must be conducted prior to scraping public personal data as well (see our previous posts on "}
{"title": "Solution architecture part 5: Designing a well-optimized web scraping solution", "first_text": "In the fifth and final post of this "}
{"title": "ScrapyRT: Turn websites into real-time APIs", "first_text": "If you\u2019ve been using Scrapy for any period of time, you know the capabilities a well-designed Scrapy spider can give you."}
{"title": "How to set up a custom proxy in Scrapy?", "first_text": "When scraping the web at a reasonable scale, you can come across a series of problems and challenges. You may want to access a website from a specific country/region. Or maybe you want to work around anti-bot solutions. Whatever the case, to overcome these obstacles you need to use and manage proxies. In this article, I'm going to cover how to set up a custom proxy inside your Scrapy spider in an easy and straightforward way. Also, we're going to discuss what are the best ways to solve your current and future proxy issues. You will learn how to do it yourself but you can also just use "}
{"title": "Solution architecture part 3: Conducting a web scraping legal review", "first_text": "In this third post in our solution architecture series, we will share with you our step-by-step process for conducting a legal review of every web scraping project we work on."}
{"title": "Visual web scraping tools: What to do when they are no longer fit for purpose?", "first_text": "Visual web scraping tools are great. They allow people with little to no technical know-how to extract data from websites with only a couple of hours of upskilling, making them great for simple lead generation, market intelligence, and competitor monitoring projects. Removing countless hours of manual entry work for sales and marketing teams, researchers, and business intelligence teams in the process."}
{"title": "Solution architecture part 4: Accessing the technical feasibility of your web scraping project", "first_text": "In the fourth post of this "}
{"title": "Learn how to configure and utilize proxies with Python requests module", "first_text": "Sending HTTP requests in Python is not necessarily easy. We have built-in modules like urllib, urllib2 to deal with HTTP requests. Also, we have third-party tools like Requests. Many developers use Requests because it is high level and designed to make it extremely easy to send HTTP requests."}
{"title": "Four use cases for online public sentiment data", "first_text": "The manual method of discovery for gauging online public sentiment towards a product, company, or industry is cursory at best, and at worst, may harm your business by providing incorrect or misleading insights."}
{"title": "The first-ever Web Data Extraction Summit!", "first_text": "The range of use cases for web data extraction is rapidly increasing and with it the necessary investment. Plus the number of websites continues to grow rapidly and is expected to exceed 2 billion by 2020."}
{"title": "Solution architecture part 2: How to define the scope of your web scraping project", "first_text": "In this second post in our solution architecture series, we will share with you our step-by-step process for data extraction requirement gathering."}
{"title": "Navigating compliance when extracting web scraped alternative data for finance", "first_text": "When it comes to using web data as alternative data for investment decision making, one topic rules them all: "}
{"title": "Proxy management: Should I build my proxy infrastructure in-house or use an off-the-shelf proxy solution?", "first_text": "Proxy management is the thorn in the side of most web scrapers. Without a robust and fully featured proxy infrastructure, you will often experience constant reliability issues and hours spent putting out proxy fires - a situation no web scraping professional wants to deal with. We, web scrapers, are interested in extracting and using web data, not managing proxies."}
{"title": "Zyte\u2019s (formerly Scrapinghub) new AI-powered Automatic Extraction API for e-commerce & article extraction", "first_text": "Today, we\u2019re delighted to announce the launch of the beta program for Zyte's (formerly Scrapinghub) new AI-powered Automatic Extraction API for automated product and article extraction."}
{"title": "A sneak peek inside Zyte Smart Proxy Manager (formerly Crawlera): The world\u2019s smartest web scraping proxy network", "first_text": "\u201cHow does Zyte Smart Proxy Manager (formerly Crawlera) work?\u201d is the most common question we get asked from customers who after struggling for months (or years) with constant proxy issues, only to have them disappear completely when they switch to Zyte Smart Proxy Manager."}
{"title": "Meet Spidermon: Zyte's (formerly Scrapinghub) battle-tested spider monitoring library [now open-sourced]", "first_text": "Your spider is developed and we are getting our structured data daily, so our job is done, right?"}
{"title": "Spidermon: Zyte's (formerly Scrapinghub) secret sauce to our data quality & reliability guarantee", "first_text": "If you know anything about Zyte (formerly Scrapinghub), you know that we are obsessed with data quality and data reliability."}
{"title": "St Patrick\u2019s day special: Finding Dublin\u2019s best pint of Guinness with web scraping", "first_text": "At Zyte (formerly Scrapinghub) we are known for our ability to help companies make mission-critical business decisions through the use of web scraped data."}
{"title": "From the creators of Scrapy: Artificial intelligence data extraction API", "first_text": "To accurately extract data from a web page, developers usually need to develop custom code for each website. This is manageable and recommended for tens or hundreds of websites and where data quality is of the utmost importance, but if you need to extract data from thousands of sites, or rapidly extract data from sites that are not yet covered by pre-existing code, this is often an insurmountable challenge."}
{"title": "Web data analysis: Exposing NFL player salaries with Python", "first_text": "Football. From throwing a pigskin with your dad, to crunching numbers to determine the probability of your favorite team winning the Super Bowl, it is a sport that's easy to grasp yet teeming with complexity. From game to game, the amount of complex data associated with every team - and every player - increases, creating a more descriptive, timely image of the League at hand."}
{"title": "The challenges e-commerce retailers face managing their web scraping proxies", "first_text": "These days web scraping amongst big e-commerce companies is ubiquitous due to the advantages that data-based decision making can bring to remain competitive in such a tight margin business."}
{"title": "Looking back at 2018", "first_text": "What a year 2018 has been for Zyte (formerly Scrapinghub)!!"}
{"title": "Do what is right not what is easy!", "first_text": "I was recently invited to speak at the IAPP Europe Data Protection Congress in Brussels about web scraping and GDPR. The panel also included Claire Fran\u00e7ois of Hunton Andrews Kurth and Peter Brown from the Information Commissioner\u2019s Office (ICO). For more information, you can check out my blog about this topic "}
{"title": "Data quality assurance for enterprise web scraping", "first_text": "When it comes to web scraping, one key element is often overlooked until it becomes a big problem."}
{"title": "What I learned as a Google Summer of Code student at Zyte (formerly Scrapinghub)", "first_text": "Google Summer of Code (GSoC) was such a great experience for students like me. I learned so much about open source communities as well as contributing to their complex projects. I also learned a great deal from my mentors, Konstantin and Cathal, about programming and software engineering practices. In my opinion, the most valuable lesson I got from GSoC was what it was like to be a Software Engineer, which prepared me to continue the pursuit of my dream career in technology."}
{"title": "GDPR compliance for web scrapers: The step-by-step guide", "first_text": "Unless you\u2019ve been living under a rock for the past few months you know that the EU\u2019s General Data Protection Regulation (GDPR) is upon us."}
{"title": "Shubber GetTogether 2018", "first_text": "It\u2019s hard to believe our annual Shubber GetTogether is already over."}
{"title": "The predictive power of web scraped product data for institutional investors: A GoPro case study", "first_text": "Investors understand the importance of high-quality information. It minimizes risk, empowers decision-making, and enables investors of all sizes to obtain alpha - like the old adage, knowing is often half the battle."}
{"title": "The rise of web data in hedge fund decision making & the importance of data quality", "first_text": "Over the past few years, there has been an explosion in the use of alternative data sources in investment decision making in hedge funds, investment banks, and private equity firms."}
{"title": "Why we created Zyte Smart Proxy Manager (formerly Crawlera)?", "first_text": "Let\u2019s face it, managing your proxy pool can be an absolute pain and the biggest bottleneck to the reliability of your web scraping!"}
{"title": "Looking back at 2016", "first_text": null}
{"title": "A faster, updated Scrapinghub (now Zyte)", "first_text": "We\u2019re very excited to announce a new look for Scrapinghub!"}
{"title": "Data for price intelligence: Lessons learned scraping 100 billion products pages", "first_text": "Web scraping can look deceptively easy these days. There are numerous open-source libraries/frameworks, visual scraping tools, and data extraction tools that make it very easy to scrape data from a website. However, when you want to scrape websites at scale things start to get very tricky, very fast. Especially when it comes to "}
{"title": "Looking back at 2017", "first_text": null}
{"title": "Want to predict Fitbit\u2019s quarterly revenue? Eagle Alpha did it using web scraped product data", "first_text": null}
{"title": "How data compliance companies are turning to web crawlers to take advantage of the GDPR business opportunity", "first_text": null}
{"title": "A sneak peek inside what hedge funds think of alternative financial data", "first_text": null}
{"title": "Scraping the Steam game store with Scrapy", "first_text": "This is a guest post from the folks over at\u00a0"}
{"title": "Deploy your Scrapy spiders from GitHub", "first_text": "Up until now, your deployment process using Scrapy Cloud has probably been something like this: code and test your spiders locally, commit and push your changes to a GitHub repository, and "}
{"title": "Do Androids dream of electric sheep?", "first_text": "It got very easy to do Machine Learning: you install an ML library like "}
{"title": "Interview: How Up Hail uses Scrapy to increase transparency", "first_text": null}
{"title": "An introduction to XPath: How to get started", "first_text": " You can use the "}
{"title": "Why promoting open data increases economic opportunities", "first_text": "\"We want to reduce the barrier of entry for people working on and with data.\""}
{"title": "Embracing the future of work: How to communicate remotely", "first_text": "What does \u201cthe Future of Work\u201d mean to you? To us, it describes how we approach life at Scrapinghub. We don't work in a traditional office (we're 100% distributed) and we allow folks the freedom to make their own schedules (you know when you work best). By\u00a0finding ways to "}
{"title": "How to deploy custom docker images for your web crawlers", "first_text": "[UPDATE]: Please see "}
{"title": "Improved Frontera: Web crawling at scale with Python 3 support", "first_text": "Python is our go-to language of choice and Python 2 is losing traction. In order to survive, older programs need to be Python 3 compatible."}
{"title": "How to run Python scripts in Scrapy Cloud", "first_text": "You can deploy, run, and maintain control over your "}
{"title": "How you can use web data to accelerate your startup", "first_text": "In just the US alone, there were "}
{"title": "How to increase sales with online reputation management", "first_text": "One negative review can cost your business up to 22% of its prospects. This was one of the sobering findings in "}
{"title": "Incremental crawls with Scrapy and DeltaFetch", "first_text": "Welcome to Scrapy Tips from the Pros! In this monthly column, we share a few tricks and hacks to help speed up your web scraping activities. As the lead Scrapy maintainers, we\u2019ve run into every obstacle you can imagine so don\u2019t worry, you\u2019re in great hands. Feel free to reach out to us on "}
{"title": "Meet Parsel: The selector library behind Scrapy", "first_text": "We eat our own spider food since Scrapy is our go-to workhorse on a daily basis. However, there are certain situations where Scrapy can be overkill and that\u2019s when we use Parsel. "}
{"title": "Scraping infinite scrolling pages", "first_text": "Welcome to Scrapy Tips from the Pros! In this monthly column, we share a few tricks and hacks to help speed up your web scraping activities. As the lead Scrapy maintainers, we\u2019ve run into every obstacle you can imagine so don\u2019t worry, you\u2019re in great hands. Feel free to reach out to us on Twitter or Facebook with any suggestions for future topics."}
{"title": "Introducing Portia2Code: Portia projects into Scrapy spiders", "first_text": null}
{"title": "Scrapely: The brains behind Portia spiders", "first_text": null}
{"title": "Improving access to Peruvian Congress bills with Scrapy", "first_text": "Many governments worldwide have laws enforcing them to publish their expenses, contracts, decisions, and so forth, on the web. This is so the general public can monitor what their representatives are doing on their behalf."}
{"title": "This month in open source at Scrapinghub (now Zyte) August 2016", "first_text": "Welcome to This Month in Open Source at Scrapinghub! In this regular column, we share all the latest updates on our open source projects including Scrapy, Splash, Portia, and Frontera."}
{"title": "Introducing Scrapy Cloud with Python 3 support", "first_text": "It\u2019s the end of an era. Python 2 is on its way out with only a few security and bug fixes forthcoming from now until its "}
{"title": "What the Suicide Squad tells us about web data", "first_text": "Web data is a bit like the Matrix. It\u2019s all around us, but not everyone knows how to use it meaningfully. So here\u2019s a brief overview of the many ways that web data can benefit you as a researcher, marketer, entrepreneur, or even multinational business owner."}
{"title": "How to crawl the web politely with Scrapy", "first_text": "The first rule of web crawling is you do not harm the website. The second rule of web crawling is you do "}
{"title": "How to build your own price monitoring tool", "first_text": "Computers are great at repetitive tasks. They don't get distracted, bored, or tired. "}
{"title": "Data extraction with Scrapy and Python 3", "first_text": "Fasten your seat belts, ladies and gentlemen: Scrapy 1.1 with Python 3 support is officially out! After a couple of months of hard work and "}
{"title": "A (not so) short story on getting decent internet access", "first_text": "This is a tale of trial, tribulation, and triumph. It is the story of how I overcame obstacles including an inconveniently placed grove of"}
{"title": "Scrapy + MonkeyLearn: Textual analysis of web data", "first_text": null}
{"title": "Introducing Scrapy Cloud 2.0", "first_text": "Scrapy Cloud has been with Scrapinghub since the beginning, but we decided some spring cleaning was in order. To that end, we\u2019re proud to announce "}
{"title": "How to debug your Scrapy spiders", "first_text": "Welcome to Scrapy Tips from the Pros! Every month we release a few tricks and hacks to help speed up your web scraping and data extraction activities. As the lead Scrapy maintainers, we have run into every obstacle you can imagine so don\u2019t worry, you\u2019re in great hands. Feel free to reach out to us on "}
{"title": "Machine learning with web scraping: New MonkeyLearn addon", "first_text": "We deal in data. Vast amounts of it. But while we\u2019ve been traditionally involved in providing you with the data that you need, we are now taking it a step further by helping you analyze it as well."}
{"title": "Scraping websites based on ViewStates with Scrapy", "first_text": "Welcome to the April Edition of "}
{"title": "Introducing the Zyte Smart Proxy Manager (formerly Crawlera) dashboard", "first_text": "\nWe\u2019ve been rolling out a lot of "}
{"title": "This month in open source at Scrapinghub June 2016", "first_text": "Welcome to This Month in Open Source at Scrapinghub! In this regular column, we share all the latest updates on our open source projects including Scrapy, Splash, Portia, and Frontera."}
{"title": "Introducing the Datasets catalog", "first_text": null}
{"title": "Portia: The open-source alternative to Kimono labs", "first_text": null}
{"title": "Scrapy tips from the pros: March 2016 edition", "first_text": null}
{"title": "Join Scrapinghub for Google Summer of Code 2016", "first_text": null}
{"title": "This month in open source at Scrapinghub (now Zyte) March 2016", "first_text": null}
{"title": "Migrate your Kimono projects to Portia", "first_text": null}
{"title": "Splash 2.0 is here with Qt 5 and Python 3", "first_text": "We\u2019re pleased to announce that Splash 2.0 is officially live after many months of hard work."}
{"title": "Scrapy tips from the pros: February 2016 edition", "first_text": "Welcome to the February Edition of "}
{"title": "How web scraping is revealing lobbying and corruption in Peru", "first_text": null}
{"title": "Web scraping to create open data", "first_text": null}
{"title": "Mapping corruption in the Panama papers with open data", "first_text": "We are at a point in the digital age where corruption is increasingly difficult to hide. "}
{"title": "Christmas eve vs new year's eve: Last-minute price inflation?", "first_text": null}
{"title": "Vizlegal: Rise of machine-readable laws and court judgments", "first_text": null}
{"title": "Scrapy tips from the pros: part 1", "first_text": " is at the heart of "}
{"title": "Chats with RINAR Solutions", "first_text": "Meet Tom\u00e1s Rinke. He is the CTO and Co-Founder of "}
{"title": "Winter sales showdown: Black Friday vs Cyber Monday vs Green Monday", "first_text": null}
{"title": "Black Friday, Cyber Monday: Are they worth it?", "first_text": "This post kicks off a series of articles that will trace the prices of some of the top gifts, gadgets, and gizmos from Black Friday through to January 2016."}
{"title": "Looking back at 2015", "first_text": "2015\u00a0has been a standout year for Scrapinghub with new developments in expanded products, noteworthy clients, and cherished partnerships. We were curious about whether we could continue our growth from 2014 and are pleased to report that our curiosity has been successfully laid to rest. So without further ado, let\u2019s dive straight into what has made 2015 a hard act to follow."}
{"title": "Web scraping finds stores guilty of price inflation", "first_text": "This is the last post ending a series of articles that have traced the prices of some of the top gifts, gadgets, and gizmos from Black Friday 2015 through to January 2016."}
{"title": "Python 3 is coming to Scrapy", "first_text": null}
{"title": "Happy anniversary: Zyte (formerly Scrapinghub) turns 5", "first_text": "Birthdays are a big deal at Zyte (formerly Scrapinghub). We always make sure to celebrate each team member on their special day, recognizing achievements and sending well-wishes for the year to come. Well, on December 15, 2015, we celebrated one of the most momentous birthdays of the year: our own. We are officially 5 years old and what an amazing 5 years it has been."}
{"title": "Distributed Frontera: Web crawling at scale", "first_text": null}
{"title": "Introducing Javascript support for Portia", "first_text": null}
{"title": "Scrapy on the road to Python 3 support", "first_text": "Scrapy is one of the few popular Python packages (almost 10k github stars) that's not yet compatible with Python 3. The team and community around it are working to make it compatible as soon as possible. Here's an overview of what has been happening so far."}
{"title": "PyCon Philippines 2015", "first_text": "Earlier this month we attended PyCon Philippines as a gold sponsor, presenting on the 2nd day. This was particularly exciting as it was the first time the whole Philippines team was together in one place and it was nice meeting each other in person!"}
{"title": "EuroPython 2015", "first_text": "EuroPython 2015 is happening this week and we\u2019re having the largest company meetup so far as a part of it, with more than 30 members from our fully remote-working team attending. The event which is held in Bilbao started on Monday and is providing great quality talks, sessions, and plenty of tasty Spanish dishes."}
{"title": "StartupChats remote working Q&A", "first_text": "Earlier this week, Scrapinghub was invited along with several other fully-distributed companies to participate in a remote working Q&A hosted by Startups Canada."}
{"title": "The road to loading JavaScript in Portia", "first_text": null}
{"title": "Aduana: Link analysis to crawl the web at scale", "first_text": "Crawling vast numbers of websites for specific types of information is impractical. Unless, that is, you prioritize what you crawl. Aduana is an experimental tool that we developed to help you do that. It\u2019s a special backend for Frontera, our tool to expedite massive crawls in parallel ("}
{"title": "Parse natural language dates with Dateparser", "first_text": "We recently released "}
{"title": "Tips for creating a cohesive company culture remotely", "first_text": ". Rapidly. This past year, we have gone from 80 to 114 Scrapinghubbers. Companies expanding so rapidly can risk losing what made them them in the first place. We don\u2019t want that to happen, especially since we have the added challenge of being an entirely distributed company with remote coworkers "}
{"title": "Link analysis algorithms explained", "first_text": "When scraping content from the web, you often crawl websites which you have no prior knowledge of. Link analysis algorithms are incredibly useful in these scenarios to guide the crawler to relevant pages."}
{"title": "Google Summer of Code 2015", "first_text": "We are very excited to be participating again this year on Google Summer of Code. After a successful experience last year where "}
{"title": "A career in remote working", "first_text": "This year I have reached a major milestone in my life, which is getting my bachelor's degree in mathematics. When I made the decision to go back to college, it was solely because my experience working at Scrapinghub, I figured out that having a math background would be a great foundation for getting into ML-related stuff."}
{"title": "Traveling tips for remote workers", "first_text": "Being free to work from wherever you feel like, no boundaries holding you to a specific place or country. This is one of the greatest advantages of working remotely, and it's leading many people to travel around the globe while completing their work. Today Claudio Salazar, a Scrapinghubber from Chile, is here to share his experiences and tips for these who seek working on the road."}
{"title": "Gender inequality across programming languages", "first_text": "Gender inequality is a hot topic in the tech industry. Over the last several years we\u2019ve gathered business profiles for our clients, and we realized this data would prove useful in identifying trends in how gender and employment relations to one another."}
{"title": "Scrape data visually with Portia and Scrapy Cloud", "first_text": null}
{"title": "Frontera: The brain behind the crawls", "first_text": "At Zyte (formerly Scrapinghub) we're always building and running large crawls\u2013last year we had 11 billion requests made on Scrapy Cloud alone. Crawling millions of pages from the internet requires more sophistication than getting a few contacts of a list, as we need to make sure that we get reliable data, up-to-date lists of item pages and are able to optimize our crawl as much as possible."}
{"title": "Using git to manage vacations in a large distributed team", "first_text": "Here at Scrapinghub we are a "}
{"title": "Aduana: Link analysis with Frontera", "first_text": "It's not uncommon to need to crawl a large number of unfamiliar websites when gathering content. Page ranking algorithms are incredibly useful in these scenarios as it can be tricky to determine which pages are relevant to the content you're looking for."}
{"title": "EuroPython, here we go!", "first_text": "We are\u00a0very excited about\u00a0EuroPython 2015!"}
{"title": "Scrapinghub (now Zyte) crawls the deep web", "first_text": "\"The easiest way to think about Memex is: How can I make the unseen seen?\""}
{"title": "Skinfer: A tool for inferring JSON schemas", "first_text": "Imagine that you have a lot of samples for a certain kind of data in JSON format. Maybe you want to have a better feel of it, know which fields appear in all records, which appear only in some and what are their types. In other words, you want to know the "}
{"title": "XPath tips from the web scraping trenches", "first_text": "In the context of web scraping, "}
{"title": "Handling JavaScript in Scrapy with Splash", "first_text": "A common roadblock when developing spiders is dealing with sites that use a heavy amount of JavaScript. Many modern websites run entirely on JavaScript and require scripts to be run in order for the page to render properly. In many cases, pages also present modals and other dialogues that need to be interacted with to show the full page. In this post, we\u2019re going to show you how you can use Splash to handle JavaScript in your Scrapy projects."}
{"title": "New changes to our Scrapy cloud platform", "first_text": "We are proud to announce some exciting changes we've introduced this week. These changes bring a much more pleasant user experience, and several new features including the addition of "}
{"title": "Introducing ScrapyRT: An API for Scrapy spiders", "first_text": "We\u2019re proud to announce our new open source project, "}
{"title": "Looking back at 2014", "first_text": "\u00a0we were looking back at the great 2013 we had and realized we would have quite a big challenge in front of us in order to have as much growth as we had during last year. So here are some highlights of the things we\u2019ve been up to during this year, let's see how well we did!"}
{"title": "Why we moved to Slack", "first_text": "We are veterans in the chat group arena. We have been using one form of another since we started Scrapinghub in 2010 and I've been personally using corporate group chats since 2004. We started Scrapinghub\u00a0using\u00a0our own hosted version of "}
{"title": "Zyte (formerly Scrapinghub): A remote working success story", "first_text": "When Zyte (formerly Scrapinghub) came into the world in 2010, one thing we wanted was for it to be a company which could be powered by a global workforce, each individual working remotely from anywhere in the world."}
{"title": "The history of Scrapinghub", "first_text": null}
{"title": "Open source at Zyte", "first_text": "Here at Zytewe love open source. We love using and contributing to it. Over these years we have open sourced a few projects, that we keep using over and over, in the hope that it will make others lives easier. Writing reusable code is harder than it sounds, but it enforces good practices such as documenting accurately, testing extensively and worrying about backwards support. In the end it produces better software, and keeps programmers happier. This is why we open source as much as we can and always deliver the complete source code to our clients, so they can run everything on their machines if they ever want or need to do so."}
{"title": "Looking back at 2013", "first_text": "This time last year Pablo and I were chatting about the previous year and what to expect in 2013. I noticed that our team had almost doubled in size in the previous year and we wondered could that possibly continue in 2013?"}
{"title": "Optimizing memory usage of Scikit-Learn models using succinct tries", "first_text": "We use the "}
{"title": "Introducing Crawlera (now Zyte Smart Proxy Manager), a smart page downloader", "first_text": "We are proud to introduce "}
{"title": "Marcos Campal is a ScrapingHubber!", "first_text": "We\u2019re excited to welcome Marcos Campal to the Scrapinghub engineering team."}
{"title": "Why MongoDB is a bad choice for storing our scraped data", "first_text": "MongoDB was used early on at Scrapinghub to store scraped data because it's convenient. Scraped data is represented as (possibly nested) records which can be serialized to JSON. The schema is not known ahead of time and may change from one job to the next. We need to support browsing, querying and downloading the stored data. This was very easy to implement using MongoDB (easier than the alternatives available a few years ago) and it worked well for some time."}
{"title": "Introducing Dash", "first_text": "We're excited to introduce "}
{"title": "Announcing Portia, the open-source visual web scraper!", "first_text": null}
{"title": "Extracting schema.org microdata using Scrapy selectors and XPath", "first_text": "In summary, we found structured data within 585 million HTML pages out of the 2.24 billion pages contained in the crawl (26%)."}
{"title": "Introducing data reviews", "first_text": "One of the things that takes more time when building a spider is reviewing the scraped data and making sure it conforms to the requirements and expectations of your client or team. This process is so time consuming that, in many cases, it ends up taking more time than writing the spider code itself, depending on how well the requirements are written. To make this process more efficient we have introduced the ability to comment data directly on Dash (Scrapinghub UI), right next to the data, instead of relying on other channels (like issue trackers, emails or chat)."}
{"title": "Finding similar items", "first_text": null}
{"title": "Scrapy 0.15 dropping support for Python 2.5", "first_text": "After a year considering it, we have decided to go ahead and drop support for Python 2.5 in Scrapy."}
{"title": "Autoscraping casts a wider net", "first_text": "We have recently started letting more users into the private beta for our "}
{"title": "Dirbot: A new example Scrapy project", "first_text": "Scrapy users have complained in the past about the lack of a pre-built example project that contains, for example, the dmoz spider described in the tutorial."}
{"title": "Scrapy 0.12 released", "first_text": "Hello everyone, we\u2019re pleased to announce the release of Scrapy 0.12!"}
{"title": "Introducing w3lib and scrapely", "first_text": "In an effort to make Scrapy code smaller and more reusable, we\u2019ve been working on splitting the Scrapy codebase into two different modules:"}
{"title": "Scrapy 0.14 released", "first_text": "After 10 months of work, and many changes, we are pleased to announce the release of Scrapy 0.14."}
{"title": "Spiders activity graphs", "first_text": "Today we are introducing a new feature called Spider activity graphs. These allow you to visualize quickly how your spiders are working, and it's a very useful tool for busy projects to find out which spiders are not working as expected."}
{"title": "How to fill login forms automatically", "first_text": "We often have to write spiders that need to login to sites, in order to scrape data from them. Our customers provide us with the site, username and password, and we do the rest."}
{"title": "Git workflow for Scrapy projects", "first_text": "Our customers often ask us what's the best workflow for working with Scrapy projects.\u00a0A popular approach we have seen and used in the past is to split the spiders folder (typically project/spiders) into two folders: project/spiders_prod and project/spiders_dev, and use the "}
{"title": "Hello, world", "first_text": "It's finally time to start a Scrapinghub blog! In the upcoming months we expect to open our private beta to new customers, launch new services, add many new features and continue to contribute to open source projects. It's about time we had a way to to tell everyone about all the great things that are happening!"}
{"title": "Spoofing your Scrapy bot IP using tsocks", "first_text": "It is well known that many websites show different content depending on the region where they\u2019re accessed. For example, some retailer sites show products available only for the region (US, Europe) of the user accessing the site."}
